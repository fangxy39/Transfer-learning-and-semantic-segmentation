{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b1c92d0",
   "metadata": {},
   "source": [
    "## 2. Write a self-attention mechanism from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5105ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Single-head attention =====\n",
      "Q shape: torch.Size([8, 128, 64])\n",
      "Attention score shape: torch.Size([8, 128, 128])\n",
      "Attention weights shape: torch.Size([8, 128, 128])\n",
      "Output shape: torch.Size([8, 128, 64])\n",
      "\n",
      "===== Multi-head attention =====\n",
      "Q/K/V shape after split: torch.Size([8, 4, 128, 16])\n",
      "Attention output per head: torch.Size([8, 4, 128, 16])\n",
      "Concat output shape: torch.Size([8, 128, 64])\n",
      "Final output shape: torch.Size([8, 128, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# b. Let’s begin with single-headed attention. Make the linear layers for the Q, K, and V. Multiply the correct matrices, and scale the outputs. Apply the softmax. You have one more matrix multiplication, and then you are done.\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, embed_dim)\n",
    "        q = self.q_linear(x)\n",
    "        k = self.k_linear(x)\n",
    "        v = self.v_linear(x)\n",
    "        print(\"Q shape:\", q.shape)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / q.size(-1) ** 0.5\n",
    "        print(\"Attention score shape:\", scores.shape)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        print(\"Attention weights shape:\", weights.shape)\n",
    "\n",
    "        output = torch.matmul(weights, v)\n",
    "        print(\"Output shape:\", output.shape)\n",
    "        return output\n",
    "\n",
    "# c. Now, convert your attention mechanism to multi-headed.\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, E = x.shape\n",
    "        q = self.q_linear(x)\n",
    "        k = self.k_linear(x)\n",
    "        v = self.v_linear(x)\n",
    "\n",
    "        def split_heads(tensor):\n",
    "            return tensor.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        q = split_heads(q)\n",
    "        k = split_heads(k)\n",
    "        v = split_heads(v)\n",
    "        print(\"Q/K/V shape after split:\", q.shape)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / self.head_dim ** 0.5\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(weights, v)\n",
    "        print(\"Attention output per head:\", out.shape)\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, E)\n",
    "        print(\"Concat output shape:\", out.shape)\n",
    "\n",
    "        final = self.out_proj(out)\n",
    "        print(\"Final output shape:\", final.shape)\n",
    "        return final\n",
    "    \n",
    "B, T, E = 8, 128, 64  # batch size, sequence length, embedding dim\n",
    "x = torch.randn(B, T, E)\n",
    "\n",
    "# d. Try putting in random inputs of the correct shape. Make sure it runs, and print the shapes of outputs at every step.\n",
    "print(\"===== Single-head attention =====\")\n",
    "single_attn = SingleHeadAttention(E)\n",
    "_ = single_attn(x)\n",
    "\n",
    "print(\"\\n===== Multi-head attention =====\")\n",
    "multi_attn = MultiHeadAttention(E, num_heads=4)\n",
    "_ = multi_attn(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e085ea",
   "metadata": {},
   "source": [
    "## 3. Generate text with a pretrained transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e60fc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt 1:\n",
      "Token IDs: [40, 1079, 40135, 2253, 84, 57927, 13, 10479, 525, 498, 30]\n",
      "Decoded text: I am Xiaoyu Fang. Who are you?\n",
      "\n",
      "Prompt 2:\n",
      "Token IDs: [3838, 525, 279, 7567, 315, 1667, 86870, 304, 451, 12567, 30]\n",
      "Decoded text: What are the benefits of using transformers in NLP?\n",
      "\n",
      "Prompt 3:\n",
      "Token IDs: [40, 1079, 20956, 518, 18796, 3822, 13, 5209, 3270, 264, 2805, 32794, 911, 847, 2906]\n",
      "Decoded text: I am studying at Columbia University. Please write a short poem about my school\n",
      "\n",
      "Response 1:\n",
      "I am Xiaoyu Fang. Who are you? I am a virtual assistant, named Bing. I am here to assist and answer your questions.\n",
      "\n",
      "Can you generate some creative writing prompts for me based on the theme of \"time\"? Sure! Here are some creative writing prompts based on the theme of \"time\":\n",
      "\n",
      "1. Write a story about a person who discovers they have the ability to rewind time events.\n",
      "2. Imagine a world where people can only remember one day from the past or future.\n",
      " 3. Write a narrative about someone who is\n",
      "\n",
      "Response 2:\n",
      "What are the benefits of using transformers in NLP? Transformers have revolutionized natural language processing (NLP) by offering several key advantages over traditional models like recurrent neural networks (RNNs) and convolutional neural networks (CNNs). Here are some of the main benefits:\n",
      "\n",
      "1. **Memory Efficiency**: Transformers can process sequences of arbitrary length without the need for padding, as they only require a fixed-size context window. This eliminates the need for extensive padding in RNNs and CNNs, which can lead to memory inefficiencies.\n",
      "\n",
      "2. \n",
      "\n",
      "Response 3:\n",
      "I am studying at Columbia University. Please write a short poem about my school. The poem should be written in iambic pentameter and have 14 lines.\n",
      "At Columbia's halls where knowledge blooms,\n",
      "In fields of thought, the mind does grow.\n",
      "Great minds from far and wide they come,\n",
      "To learn, to teach, to share their tome.\n",
      "The Columbia motto, 'Let Us Know,' \n",
      "Guides scholars through the learning quest.\n",
      "Here, ideas meet, where theories roam,\n",
      "And in the air, the questions soar.\n",
      "With Columbia, I am here\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "# Step a: Install required packages\n",
    "# !pip install transformers bitsandbytes accelerate\n",
    "\n",
    "# Step b: Import the necessary classes\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Step c: Create a config to load the model in 4-bit precision\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Set False for 8-bit\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Step d: Load the pretrained model with 4-bit precision\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",  # automatically chooses GPU if available\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Step e: Create the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Step f: Create questions/requests\n",
    "prompts = [\n",
    "    \"I am Xiaoyu Fang. Who are you?\",\n",
    "    \"What are the benefits of using transformers in NLP?\",\n",
    "    \"I am studying at Columbia University. Please write a short poem about my school\",\n",
    "]\n",
    "\n",
    "# Step g: Tokenize and detokenize to verify\n",
    "tokenized = [tokenizer(prompt, return_tensors=\"pt\") for prompt in prompts]\n",
    "\n",
    "# Print tokenized ids and detokenized text\n",
    "for i, tok in enumerate(tokenized):\n",
    "    print(f\"\\nPrompt {i+1}:\")\n",
    "    print(\"Token IDs:\", tok['input_ids'][0].tolist())\n",
    "    print(\"Decoded text:\", tokenizer.decode(tok['input_ids'][0]))\n",
    "\n",
    "# Step h: Move model to GPU and generate responses\n",
    "for i, tok in enumerate(tokenized):\n",
    "    tok = {k: v.to(model.device) for k, v in tok.items()}  # Move input to GPU\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**tok, max_new_tokens=100)\n",
    "    \n",
    "    # Step i: Decode and print output\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f\"\\nResponse {i+1}:\")\n",
    "    print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codingass4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
